# ğŸš€ AI Appointment Scheduler Assistant

## ğŸ“– Description

The AI Appointment Scheduler Assistant converts natural-language or image-based inputs into clean, structured appointment data using Google Gemini AI. It processes user text or OCR-extracted content, identifies key appointment details (date, time, department), normalizes them into ISO formats, applies guardrails for ambiguous cases, and returns a final validated appointment JSON.

## ğŸ§° Tech Stack Used

**Frontend:** React, Vite, TailwindCSS  
**Backend:** Node.js, Express, Multer, Joi, Luxon  
**AI:** Gemini Flash (NLP), Gemini Vision (OCR)  
**Deployment:** Render, GitHub  

---

## âœ¨ Features
- ğŸ“· **OCR Support:** Extract text from handwritten notes or screenshots  
- ğŸ” **Entity Extraction:** Identify date, time, and department  
- ğŸ—“ **Normalization:** Convert phrases like *"next Friday at 3pm"* to ISO datetime  
- âš ï¸ **Guardrails:** Returns `needs_clarification` for ambiguous inputs  
- ğŸ“¦ **Final JSON Output:** Structured appointment object  
- ğŸ¨ **Modern UI:** Built with TailwindCSS  

---

## ğŸ—ï¸ Architecture
```text
ai-appointment-scheduler/
â”‚
â”œâ”€ backend/
â”‚  â”œâ”€ package.json
â”‚  â”œâ”€ .env.example
â”‚  â”œâ”€ .gitignore
â”‚  â””â”€ src/
â”‚     â”œâ”€ app.js
â”‚     â”œâ”€ server.js
â”‚     â”‚
â”‚     â”œâ”€ routes/
â”‚     â”‚  â””â”€ appointmentRoutes.js
â”‚     â”‚
â”‚     â”œâ”€ controllers/
â”‚     â”‚  â””â”€ appointmentController.js
â”‚     â”‚
â”‚     â”œâ”€ services/
â”‚     â”‚  â”œâ”€ geminiClient.js
â”‚     â”‚  â”œâ”€ ocrService.js
â”‚     â”‚  â”œâ”€ nlpService.js
â”‚     â”‚  â””â”€ normalizationService.js
â”‚     â”‚
â”‚     â””â”€ middleware/
â”‚        â””â”€ errorMiddleware.js
â”‚
â”œâ”€ frontend/
â”‚  â”œâ”€ public/
â”‚  â”‚  â””â”€ vite.svg
â”‚  â”‚
â”‚  â”œâ”€ package.json
â”‚  â”œâ”€ index.html
â”‚  â”œâ”€ vite.config.js
â”‚  â”‚
â”‚  â””â”€ src/
â”‚     â”œâ”€ main.jsx
â”‚     â”œâ”€ App.jsx
â”‚     â”‚
â”‚     â”œâ”€ api/
â”‚     â”‚  â””â”€ client.js
â”‚     â”‚
â”‚     â””â”€ components/
â”‚        â”œâ”€ AppointmentForm.jsx
â”‚        â”œâ”€ JsonViewer.jsx
â”‚        â””â”€ ResultPanel.jsx
â”‚
â””â”€ README.md

```

## âš™ï¸ Installation / Setup

### 1ï¸âƒ£ Clone project
```bash
git clone https://github.com/Udaybisone/ai-appointment-scheduler.git
cd ai-appointment-scheduler
```
### 2ï¸âƒ£ Setup Backend Environment variables 
```bash
PORT=5000
GOOGLE_API_KEY=AIzaSyAyQdNxjD3YwmcHNWgltVYbkph6JC1Ry0w
```
### 3ï¸âƒ£ Setup Frontend
```bash
cd frontend
npm install
npm run dev
```
### 4ï¸âƒ£ Setup Backend
```bash
cd backend
npm install
npm run dev
```

## ğŸ”„ API Calls Flow

1. **User submits input**  
   - Either text or an uploaded image.

2. **Frontend sends request**  
   - Sends a `POST /api/appointments/parse` request with `mode`, `text` or `image`.

3. **Backend receives request**  
   - Multer handles image upload (if any).  
   - Controller starts the AI pipeline.

4. **Step 1: OCR / Text Extraction**  
   - If text â†’ use directly.  
   - If image â†’ Gemini Vision extracts text.

5. **Step 2: Entity Extraction**  
   - Gemini Flash extracts:  
     `date_phrase`, `time_phrase`, `department`.

6. **Step 3: Normalization**  
   - Phrases converted into ISO date/time + canonical department.

7. **Guardrail Check**  
   - If any ambiguity â†’ return:  
     `{ "status": "needs_clarification" }`.

8. **Step 4: Final JSON**  
   - Build structured appointment response.

9. **Frontend displays all steps**  
   - Shows step1 â†’ step2 â†’ step3 â†’ final output. 

---

## ğŸ“˜ API usage examples

### ğŸ“¡ API Endpoints

#### **POST /api/appointments/parse**
Parses user input (text or image) and returns the full AI pipeline output.

**Request (text mode):**
```json
{
  "mode": "text",
  "text": "Book dentist next Friday at 3pm"
}
```

**Response (success):**
```json
{
  "final": {
    "appointment": {
      "department": "Dentistry",
      "date": "2025-09-26",
      "time": "15:00",
      "tz": "Asia/Kolkata"
    },
    "status": "ok"
  }
}

```
**Response (if clarification needed):**
```json
{
  "status": "needs_clarification",
  "message": "Ambiguous date/time or department"
}
```

---

## ğŸ¤– Effective Use of AI for Chaining & Validation

I didnâ€™t just call the AI once and trust whatever it returned.  
The whole project is built around **chaining multiple AI steps** and **validating each step** before moving on.

Hereâ€™s how I used AI more carefully instead of blindly:

- **Step-wise AI calls**  
  - 1st call: OCR (if image) â†’ get raw text  
  - 2nd call: Entity extraction â†’ get `date_phrase`, `time_phrase`, `department`  
  - 3rd call: Normalization â†’ turn those phrases into final ISO `date`, `time`, `tz`, `department_canonical`

- **Structured JSON at each step**  
  Each AI call is asked to return **strict JSON**, which I then parse in the backend.  
  If the JSON is invalid or fields are missing, I donâ€™t continue the pipeline.

- **Validation & guardrails**  
  - If the model canâ€™t confidently find date/time/department, I mark it as `needs_clarification`.  
  - Normalization also has its own check: if the phrase is too vague (like â€œthis weekendâ€), I donâ€™t pretend to know the answer.  
  - In these cases, the API returns:
    ```json
    { "status": "needs_clarification", "message": "Ambiguous date/time or department" }
    ```

- **Separation of concerns**  
  - OCR, entity extraction, and normalization are in **separate service files**.  
  - This makes it easy to test each step individually and see where something went wrong.

Overall, AI is used as a **reasoning engine** inside a controlled pipeline.  
The backend is responsible for checking, validating, and deciding whether the result is safe to use or if the user should be asked for clarification.

---

